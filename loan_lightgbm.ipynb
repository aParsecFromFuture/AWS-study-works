{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0425fba",
   "metadata": {},
   "source": [
    "### Setting Up AWS Environment for SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2402815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Getting the execution role for SageMaker\n",
    "aws_role = get_execution_role()\n",
    "\n",
    "# Getting the AWS region using the boto3 session\n",
    "aws_region = boto3.Session().region_name\n",
    "\n",
    "# Creating a SageMaker session\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Getting the default S3 bucket associated with the SageMaker session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Defining a prefix for the S3 location\n",
    "prefix = \"demo-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b33d5f",
   "metadata": {},
   "source": [
    "### Downloading Loan Dataset from AWS S3 via \"download_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4acf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating an S3 client object\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Downloading the training dataset file from the specified S3 bucket and saving it locally as 'train.csv'\n",
    "s3.download_file(\"farukcan-loan-eligibility\", \"demo-1/source/loan-train.csv\", \"train.csv\")\n",
    "\n",
    "# Downloading the testing dataset file from the specified S3 bucket and saving it locally as 'test.csv'\n",
    "s3.download_file(\"farukcan-loan-eligibility\", \"demo-1/source/loan-test.csv\", \"test.csv\")\n",
    "\n",
    "# Reading the training dataset from the local file 'train.csv' into a pandas DataFrame\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Reading the testing dataset from the local file 'test.csv' into a pandas DataFrame\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bcce4",
   "metadata": {},
   "source": [
    "### Downloading Loan Dataset from AWS S3 via \"download_fileobj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de24ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# Create byte streams to store downloaded data\n",
    "train_buffer = io.BytesIO()\n",
    "test_buffer = io.BytesIO()\n",
    "\n",
    "# Initialize boto3 client for accessing AWS S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Download training data from S3 bucket \"farukcan-loan-eligibility\" and store it in train_buffer\n",
    "s3.download_fileobj(\"farukcan-loan-eligibility\", \"demo-1/source/loan-train.csv\", train_buffer)\n",
    "\n",
    "# Download testing data from S3 bucket \"farukcan-loan-eligibility\" and store it in test_buffer\n",
    "s3.download_fileobj(\"farukcan-loan-eligibility\", \"demo-1/source/loan-test.csv\", test_buffer)\n",
    "\n",
    "# Reset the buffer position to the beginning to start reading data from the beginning\n",
    "train_buffer.seek(0)\n",
    "\n",
    "# Read the training data into a Pandas DataFrame from the train_buffer\n",
    "train = pd.read_csv(train_buffer)\n",
    "\n",
    "# Reset the buffer position to the beginning to start reading data from the beginning\n",
    "test_buffer.seek(0)\n",
    "\n",
    "# Read the testing data into a Pandas DataFrame from the test_buffer\n",
    "test = pd.read_csv(test_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b724d6",
   "metadata": {},
   "source": [
    "### Data Preparation: Preprocessing Loan Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59770f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping the 'Loan_Status' column to binary values: 'Y' -> 1, 'N' -> 0, and storing the result in a new 'target' column\n",
    "train[\"target\"] = train[\"Loan_Status\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "# Dropping unnecessary columns 'Loan_Status' and 'Loan_ID' from the training dataset\n",
    "train = train.drop(columns=[\"Loan_Status\", \"Loan_ID\"])\n",
    "\n",
    "# Reordering the columns so that the 'target' column is the first column in the DataFrame\n",
    "train = train[[\"target\"] + train.columns[:-1].tolist()]\n",
    "\n",
    "# Dropping the 'Loan_ID' column from the testing dataset\n",
    "test = test.drop(columns=[\"Loan_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b79189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target Gender Married Dependents     Education Self_Employed  \\\n",
       "0       1   Male      No          0      Graduate            No   \n",
       "1       0   Male     Yes          1      Graduate            No   \n",
       "2       1   Male     Yes          0      Graduate           Yes   \n",
       "3       1   Male     Yes          0  Not Graduate            No   \n",
       "4       1   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area  \n",
       "0             1.0         Urban  \n",
       "1             1.0         Rural  \n",
       "2             1.0         Urban  \n",
       "3             1.0         Urban  \n",
       "4             1.0         Urban  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e58579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3076</td>\n",
       "      <td>1500</td>\n",
       "      <td>126.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5000</td>\n",
       "      <td>1800</td>\n",
       "      <td>208.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2340</td>\n",
       "      <td>2546</td>\n",
       "      <td>100.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3276</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n",
       "0   Male     Yes          0      Graduate            No             5720   \n",
       "1   Male     Yes          1      Graduate            No             3076   \n",
       "2   Male     Yes          2      Graduate            No             5000   \n",
       "3   Male     Yes          2      Graduate            No             2340   \n",
       "4   Male      No          0  Not Graduate            No             3276   \n",
       "\n",
       "   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
       "0                  0       110.0             360.0             1.0   \n",
       "1               1500       126.0             360.0             1.0   \n",
       "2               1800       208.0             360.0             1.0   \n",
       "3               2546       100.0             360.0             NaN   \n",
       "4                  0        78.0             360.0             1.0   \n",
       "\n",
       "  Property_Area  \n",
       "0         Urban  \n",
       "1         Urban  \n",
       "2         Urban  \n",
       "3         Urban  \n",
       "4         Urban  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f50f7",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8d9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split function from the sklearn.model_selection module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the training dataset into train and validation sets with a 70-30 ratio\n",
    "# Stratifying the split based on the 'target' column to maintain class distribution in both sets\n",
    "train, valid = train_test_split(\n",
    "    train, test_size=0.3, shuffle=True, stratify=train.target)\n",
    "\n",
    "# Resetting the index of both train and validation sets after the split\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fcb69b",
   "metadata": {},
   "source": [
    "### Handling Missing Values: Imputation for Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f5377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the SimpleImputer class from the sklearn.impute module\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating SimpleImputer objects for numerical and categorical features\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Defining column names for categorical and numerical features\n",
    "X_cat_column_names = [\"Gender\", \"Married\", \"Dependents\", \"Education\", \"Self_Employed\", \"Property_Area\"]\n",
    "X_num_column_names = [\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\", \"Credit_History\"]\n",
    "\n",
    "# Imputing missing values in numerical features using mean strategy\n",
    "train[X_num_column_names] = num_imputer.fit_transform(train[X_num_column_names])\n",
    "valid[X_num_column_names] = num_imputer.transform(valid[X_num_column_names])\n",
    "test[X_num_column_names] = num_imputer.transform(test[X_num_column_names])\n",
    "\n",
    "# Imputing missing values in categorical features using most frequent strategy\n",
    "train[X_cat_column_names] = cat_imputer.fit_transform(train[X_cat_column_names])\n",
    "valid[X_cat_column_names] = cat_imputer.transform(valid[X_cat_column_names])\n",
    "test[X_cat_column_names] = cat_imputer.transform(test[X_cat_column_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12751753",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features: Ordinal Encoding for Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fe9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the OrdinalEncoder class from the sklearn.preprocessing module\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Creating an OrdinalEncoder object\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Defining column names for ordinal features to be encoded\n",
    "ordinal_col_names = [\"Dependents\", \"Gender\", \"Married\", \"Education\", \"Self_Employed\"]\n",
    "\n",
    "# Encoding ordinal features in train, validation, and test sets\n",
    "for col_name in ordinal_col_names:\n",
    "    train[col_name] = encoder.fit_transform(train[[col_name]])\n",
    "    valid[col_name] = encoder.transform(valid[[col_name]])\n",
    "    test[col_name] = encoder.transform(test[[col_name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf1c2f",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features: One-Hot Encoding for Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fa0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the OneHotEncoder class from the sklearn.preprocessing module\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating a OneHotEncoder object with sparse_output set to False for dense output\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Defining column names for categorical features to be one-hot encoded\n",
    "onehot_col_names = [\"Property_Area\"]\n",
    "\n",
    "# Encoding categorical features into one-hot representations for train set\n",
    "cat_encoded = encoder.fit_transform(train[onehot_col_names])\n",
    "cat_encoded = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out())\n",
    "train = pd.concat([train, cat_encoded], axis=1)\n",
    "train = train.drop(columns=onehot_col_names)\n",
    "\n",
    "# Encoding categorical features into one-hot representations for validation set\n",
    "cat_encoded = encoder.transform(valid[onehot_col_names])\n",
    "cat_encoded = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out())\n",
    "valid = pd.concat([valid, cat_encoded], axis=1)\n",
    "valid = valid.drop(columns=onehot_col_names)\n",
    "\n",
    "# Encoding categorical features into one-hot representations for test set\n",
    "cat_encoded = encoder.transform(test[onehot_col_names])\n",
    "cat_encoded = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out())\n",
    "test = pd.concat([test, cat_encoded], axis=1)\n",
    "test = test.drop(columns=onehot_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6033f3",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction: Truncated Singular Value Decomposition (SVD) for Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1326678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Explained Variance Ratio: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Importing the TruncatedSVD class from the sklearn.decomposition module\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Creating a TruncatedSVD object with specified parameters\n",
    "svd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n",
    "\n",
    "# Extracting column names for features from the train dataset\n",
    "X_column_names = train.columns[1:]\n",
    "\n",
    "# Fitting the TruncatedSVD model to the training data\n",
    "svd.fit(train[X_column_names])\n",
    "\n",
    "# Printing the cumulative explained variance ratio of the SVD model\n",
    "print(\"SVD Explained Variance Ratio: {:.4f}\".format(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "# Transforming the train, validation, and test sets using the fitted SVD model\n",
    "X_train = svd.transform(train[X_column_names])\n",
    "X_valid = svd.transform(valid[X_column_names])\n",
    "X_test = svd.transform(test[X_column_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1a7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2487.498399</td>\n",
       "      <td>-359.241196</td>\n",
       "      <td>-411.955677</td>\n",
       "      <td>-56.634263</td>\n",
       "      <td>-0.737743</td>\n",
       "      <td>0.717549</td>\n",
       "      <td>-0.178683</td>\n",
       "      <td>0.418683</td>\n",
       "      <td>-0.106411</td>\n",
       "      <td>-0.123231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5197.870114</td>\n",
       "      <td>6517.924960</td>\n",
       "      <td>-43.027121</td>\n",
       "      <td>7.884235</td>\n",
       "      <td>-1.049943</td>\n",
       "      <td>-0.568556</td>\n",
       "      <td>-0.472483</td>\n",
       "      <td>0.340707</td>\n",
       "      <td>0.053364</td>\n",
       "      <td>-0.280527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2888.882363</td>\n",
       "      <td>1219.123088</td>\n",
       "      <td>-237.047320</td>\n",
       "      <td>-45.562903</td>\n",
       "      <td>-0.747907</td>\n",
       "      <td>-0.563071</td>\n",
       "      <td>-0.512018</td>\n",
       "      <td>0.790443</td>\n",
       "      <td>-0.021422</td>\n",
       "      <td>-0.027585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4851.581591</td>\n",
       "      <td>1387.598444</td>\n",
       "      <td>-185.867717</td>\n",
       "      <td>12.796768</td>\n",
       "      <td>1.293758</td>\n",
       "      <td>0.694074</td>\n",
       "      <td>-0.201757</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>-0.268680</td>\n",
       "      <td>-0.104215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3512.921528</td>\n",
       "      <td>-516.911541</td>\n",
       "      <td>-269.820123</td>\n",
       "      <td>-14.417922</td>\n",
       "      <td>0.196540</td>\n",
       "      <td>0.761185</td>\n",
       "      <td>-0.180114</td>\n",
       "      <td>0.997915</td>\n",
       "      <td>-0.017266</td>\n",
       "      <td>-0.111557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target            0            1           2          3         4  \\\n",
       "0       0  2487.498399  -359.241196 -411.955677 -56.634263 -0.737743   \n",
       "1       1  5197.870114  6517.924960  -43.027121   7.884235 -1.049943   \n",
       "2       1  2888.882363  1219.123088 -237.047320 -45.562903 -0.747907   \n",
       "3       1  4851.581591  1387.598444 -185.867717  12.796768  1.293758   \n",
       "4       1  3512.921528  -516.911541 -269.820123 -14.417922  0.196540   \n",
       "\n",
       "          5         6         7         8         9  \n",
       "0  0.717549 -0.178683  0.418683 -0.106411 -0.123231  \n",
       "1 -0.568556 -0.472483  0.340707  0.053364 -0.280527  \n",
       "2 -0.563071 -0.512018  0.790443 -0.021422 -0.027585  \n",
       "3  0.694074 -0.201757  0.013021 -0.268680 -0.104215  \n",
       "4  0.761185 -0.180114  0.997915 -0.017266 -0.111557  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train[\"target\"], pd.DataFrame(X_train)], axis=1)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8252eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2964.965785</td>\n",
       "      <td>-434.746709</td>\n",
       "      <td>-279.872157</td>\n",
       "      <td>-34.026609</td>\n",
       "      <td>-0.582896</td>\n",
       "      <td>0.694165</td>\n",
       "      <td>-0.136328</td>\n",
       "      <td>0.329779</td>\n",
       "      <td>0.017270</td>\n",
       "      <td>0.530203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3596.275090</td>\n",
       "      <td>2524.382992</td>\n",
       "      <td>-192.354337</td>\n",
       "      <td>-6.117200</td>\n",
       "      <td>-0.628901</td>\n",
       "      <td>-0.601735</td>\n",
       "      <td>-0.555460</td>\n",
       "      <td>-0.449758</td>\n",
       "      <td>-0.293870</td>\n",
       "      <td>0.072372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>651.685722</td>\n",
       "      <td>2862.120073</td>\n",
       "      <td>-274.172065</td>\n",
       "      <td>-24.515829</td>\n",
       "      <td>1.494883</td>\n",
       "      <td>0.745779</td>\n",
       "      <td>-0.146195</td>\n",
       "      <td>0.261706</td>\n",
       "      <td>0.634675</td>\n",
       "      <td>0.113011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5636.406467</td>\n",
       "      <td>437.709456</td>\n",
       "      <td>-187.003017</td>\n",
       "      <td>21.597709</td>\n",
       "      <td>0.237222</td>\n",
       "      <td>-0.239987</td>\n",
       "      <td>0.792219</td>\n",
       "      <td>-0.261983</td>\n",
       "      <td>-0.372239</td>\n",
       "      <td>-0.073717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17077.343150</td>\n",
       "      <td>-2557.927256</td>\n",
       "      <td>112.589630</td>\n",
       "      <td>8.722956</td>\n",
       "      <td>-1.205531</td>\n",
       "      <td>0.804555</td>\n",
       "      <td>-0.184410</td>\n",
       "      <td>0.612138</td>\n",
       "      <td>1.124751</td>\n",
       "      <td>-0.234933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target             0            1           2          3         4  \\\n",
       "0       0   2964.965785  -434.746709 -279.872157 -34.026609 -0.582896   \n",
       "1       1   3596.275090  2524.382992 -192.354337  -6.117200 -0.628901   \n",
       "2       1    651.685722  2862.120073 -274.172065 -24.515829  1.494883   \n",
       "3       1   5636.406467   437.709456 -187.003017  21.597709  0.237222   \n",
       "4       1  17077.343150 -2557.927256  112.589630   8.722956 -1.205531   \n",
       "\n",
       "          5         6         7         8         9  \n",
       "0  0.694165 -0.136328  0.329779  0.017270  0.530203  \n",
       "1 -0.601735 -0.555460 -0.449758 -0.293870  0.072372  \n",
       "2  0.745779 -0.146195  0.261706  0.634675  0.113011  \n",
       "3 -0.239987  0.792219 -0.261983 -0.372239 -0.073717  \n",
       "4  0.804555 -0.184410  0.612138  1.124751 -0.234933  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = pd.concat([valid[\"target\"], pd.DataFrame(X_valid)], axis=1)\n",
    "valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97f91da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5666.572894</td>\n",
       "      <td>-841.260253</td>\n",
       "      <td>-203.571304</td>\n",
       "      <td>-29.901946</td>\n",
       "      <td>-0.531782</td>\n",
       "      <td>-0.632406</td>\n",
       "      <td>-0.595646</td>\n",
       "      <td>-0.510643</td>\n",
       "      <td>-0.332050</td>\n",
       "      <td>0.040606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3275.335166</td>\n",
       "      <td>1035.199335</td>\n",
       "      <td>-240.309458</td>\n",
       "      <td>-7.012766</td>\n",
       "      <td>0.395590</td>\n",
       "      <td>-0.653382</td>\n",
       "      <td>-0.554321</td>\n",
       "      <td>-0.236083</td>\n",
       "      <td>-0.334924</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5222.678166</td>\n",
       "      <td>1046.376258</td>\n",
       "      <td>-195.539623</td>\n",
       "      <td>57.997963</td>\n",
       "      <td>1.072440</td>\n",
       "      <td>-0.674123</td>\n",
       "      <td>-0.523643</td>\n",
       "      <td>0.102895</td>\n",
       "      <td>-0.274271</td>\n",
       "      <td>-0.046935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2702.117565</td>\n",
       "      <td>2178.385118</td>\n",
       "      <td>-222.234255</td>\n",
       "      <td>-33.639578</td>\n",
       "      <td>1.442499</td>\n",
       "      <td>-0.687172</td>\n",
       "      <td>-0.525264</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>-0.351920</td>\n",
       "      <td>0.091784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3250.444819</td>\n",
       "      <td>-477.883726</td>\n",
       "      <td>-268.390820</td>\n",
       "      <td>-45.432145</td>\n",
       "      <td>-0.525566</td>\n",
       "      <td>-0.672907</td>\n",
       "      <td>-0.497303</td>\n",
       "      <td>-0.084201</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>-0.348250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1           2          3         4         5  \\\n",
       "0  5666.572894  -841.260253 -203.571304 -29.901946 -0.531782 -0.632406   \n",
       "1  3275.335166  1035.199335 -240.309458  -7.012766  0.395590 -0.653382   \n",
       "2  5222.678166  1046.376258 -195.539623  57.997963  1.072440 -0.674123   \n",
       "3  2702.117565  2178.385118 -222.234255 -33.639578  1.442499 -0.687172   \n",
       "4  3250.444819  -477.883726 -268.390820 -45.432145 -0.525566 -0.672907   \n",
       "\n",
       "          6         7         8         9  \n",
       "0 -0.595646 -0.510643 -0.332050  0.040606  \n",
       "1 -0.554321 -0.236083 -0.334924  0.000871  \n",
       "2 -0.523643  0.102895 -0.274271 -0.046935  \n",
       "3 -0.525264  0.009838 -0.351920  0.091784  \n",
       "4 -0.497303 -0.084201  0.850575 -0.348250  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(X_test)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25593c",
   "metadata": {},
   "source": [
    "### Uploading Preprocessed Data to S3 Bucket via \"upload_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02b71b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving preprocessed data to local CSV files\n",
    "train.to_csv(\"train_preprocessed.csv\", index=False, header=False)\n",
    "valid.to_csv(\"valid_preprocessed.csv\", index=False, header=False)\n",
    "test.to_csv(\"test_preprocessed.csv\", index=False, header=False)\n",
    "\n",
    "# Uploading the preprocessed training data to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/data.csv\")\n",
    ").upload_file(\"train_preprocessed.csv\")\n",
    "\n",
    "# Uploading the preprocessed validation data to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"valid/data.csv\")\n",
    ").upload_file(\"valid_preprocessed.csv\")\n",
    "\n",
    "# Uploading the preprocessed testing data to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/data.csv\")\n",
    ").upload_file(\"test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9a8d4",
   "metadata": {},
   "source": [
    "### Uploading Preprocessed Data to S3 Bucket via \"upload_fileobj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e47a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "# Convert dataframes to CSV format and encode as bytes\n",
    "train_buffer = io.BytesIO(train.to_csv(index=False, header=False).encode(\"utf-8\"))\n",
    "valid_buffer = io.BytesIO(valid.to_csv(index=False, header=False).encode(\"utf-8\"))\n",
    "test_buffer = io.BytesIO(test.to_csv(index=False, header=False).encode(\"utf-8\"))\n",
    "\n",
    "# Upload the training data buffer to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/data.csv\")\n",
    ").upload_fileobj(train_buffer)\n",
    "\n",
    "# Upload the validation data buffer to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"valid/data.csv\")\n",
    ").upload_fileobj(valid_buffer)\n",
    "\n",
    "# Upload the test data buffer to S3\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/data.csv\")\n",
    ").upload_fileobj(test_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cfb49",
   "metadata": {},
   "source": [
    "### Retrieving SageMaker Resources for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b41bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "# Defining parameters for training the model\n",
    "train_model_id = \"lightgbm-classification-model\"\n",
    "train_model_version = \"2.1.0\"  # Use the latest version of the model\n",
    "train_scope = \"training\"\n",
    "train_instance_type = \"ml.m5.xlarge\"  # Instance type for training\n",
    "\n",
    "# Retrieving the Docker image URI for training\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,  # Use default region\n",
    "    framework=None,  # No specific framework required\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=train_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieving the source script URI for training\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    script_scope=train_scope,\n",
    ")\n",
    "\n",
    "# Retrieving the URI of the pre-trained model tarball for further fine-tuning\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    model_scope=train_scope,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79980e0b",
   "metadata": {},
   "source": [
    "### Retrieving Default Hyperparameters and Setting Custom Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b1b71db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_boost_round': '100',\n",
       " 'early_stopping_rounds': '30',\n",
       " 'metric': 'auto',\n",
       " 'learning_rate': '0.1',\n",
       " 'num_leaves': '67',\n",
       " 'feature_fraction': '0.74',\n",
       " 'bagging_fraction': '0.53',\n",
       " 'bagging_freq': '5',\n",
       " 'max_depth': '11',\n",
       " 'min_data_in_leaf': '26',\n",
       " 'max_delta_step': '0.0',\n",
       " 'lambda_l1': '0.0',\n",
       " 'lambda_l2': '0.0',\n",
       " 'boosting': 'gbdt',\n",
       " 'min_gain_to_split': '0.0',\n",
       " 'scale_pos_weight': '1.0',\n",
       " 'tree_learner': 'serial',\n",
       " 'feature_fraction_bynode': '1.0',\n",
       " 'is_unbalance': 'False',\n",
       " 'max_bin': '255',\n",
       " 'num_threads': '0',\n",
       " 'verbosity': '1',\n",
       " 'use_dask': 'False'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary functions from SageMaker\n",
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieving default hyperparameters for the specified model\n",
    "params = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version)\n",
    "\n",
    "# Setting a custom value for the number of boosting rounds and learning rate\n",
    "params[\"num_boost_round\"] = '100'\n",
    "params[\"learning_rate\"] = '0.1'\n",
    "\n",
    "# Displaying the updated hyperparameters\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e91d4",
   "metadata": {},
   "source": [
    "### Setting Up SageMaker Estimator for Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ccc20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes from SageMaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Defining the S3 output location for the trained model\n",
    "s3_output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "# Creating an Estimator object for training\n",
    "estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=train_instance_type,\n",
    "    max_run=1000,\n",
    "    hyperparameters=params,\n",
    "    output_path=s3_output_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e39f1a",
   "metadata": {},
   "source": [
    "### Initiating Training Job with Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d190a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: my-first-example-2024-03-27-10-14-09-762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 10:14:09 Starting - Starting the training job......\n",
      "2024-03-27 10:14:50 Starting - Preparing the instances for training...\n",
      "2024-03-27 10:15:30 Downloading - Downloading the training image...\n",
      "2024-03-27 10:16:01 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:08,539 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:08,541 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:08,552 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:08,554 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:08,781 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->dask==2022.12.1->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->distributed==2022.12.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: toolz, locket, partd, HeapDict, zict, tblib, sortedcontainers, msgpack, dask, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[34mSuccessfully installed HeapDict-1.0.1 dask-2022.12.1 distributed-2022.12.1 graphviz-0.17 lightgbm-3.3.3 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:11,574 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:11,586 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:11,596 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:11,604 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"early_stopping_rounds\": \"30\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.1\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auto\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"100\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"serial\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"my-first-example-2024-03-27-10-14-09-762\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-eu-north-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.1/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.1\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"100\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"serial\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-eu-north-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.1/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.1\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"100\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"serial\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"my-first-example-2024-03-27-10-14-09-762\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-eu-north-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.1/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--early_stopping_rounds\",\"30\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.1\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auto\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"100\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"serial\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=30\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC=auto\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=100\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_LEARNER=serial\u001b[0m\n",
      "\u001b[34mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --early_stopping_rounds 30 --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.1 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auto --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 100 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner serial --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading data\u001b[0m\n",
      "\u001b[34mINFO:root:Found data in the validation channel. Reading the train and validation data from the training and validation channel, respectively.\u001b[0m\n",
      "\u001b[34mdata frame ['/opt/ml/input/data/train/data.csv']???      0             1            2   ...        8         9         10\u001b[0m\n",
      "\u001b[34m0     1   2157.776171  -264.022176  ...  0.377877  0.781250 -0.253378\u001b[0m\n",
      "\u001b[34m1     1   2035.894854  1484.015721  ... -0.314813 -0.297224 -0.018597\u001b[0m\n",
      "\u001b[34m2     1  13156.825521 -1697.622861  ...  1.130874 -0.185531 -0.195410\u001b[0m\n",
      "\u001b[34m3     1   2385.172262  1114.869839  ... -0.073861 -0.309866  0.099569\u001b[0m\n",
      "\u001b[34m4     0   3662.059324  3568.769721  ...  0.204411 -0.131478  0.671185\u001b[0m\n",
      "\u001b[34m..   ..           ...          ...  ...       ...       ...       ...\u001b[0m\n",
      "\u001b[34m424   0   4411.951664   184.006460  ... -0.838212  0.706902 -0.086601\u001b[0m\n",
      "\u001b[34m425   0   3527.818593  -442.323897  ...  0.704423 -0.143978  0.535222\u001b[0m\n",
      "\u001b[34m426   1  21520.632662  3937.031421  ...  0.156676  0.065747  0.228943\u001b[0m\n",
      "\u001b[34m427   0   2700.034836  1523.544024  ... -0.575793 -0.399170 -0.033152\u001b[0m\n",
      "\u001b[34m428   1   4075.931769  -522.384469  ... -0.458210 -0.451226 -0.482295\u001b[0m\n",
      "\u001b[34m[429 rows x 11 columns]\u001b[0m\n",
      "\u001b[34mdata frame ['/opt/ml/input/data/validation/data.csv']???      0             1             2   ...        8         9         10\u001b[0m\n",
      "\u001b[34m0     0   2232.539866   -268.652050  ...  0.507582 -0.036312  0.874827\u001b[0m\n",
      "\u001b[34m1     1   1382.709670   2631.059728  ... -0.580530 -0.426962 -0.038921\u001b[0m\n",
      "\u001b[34m2     1   2705.921243   -335.437429  ...  0.012010 -0.069108 -0.300073\u001b[0m\n",
      "\u001b[34m3     1   3571.485726   1566.986665  ... -0.319723  0.641840  0.071421\u001b[0m\n",
      "\u001b[34m4     0   4968.465321   -629.571596  ...  0.788433 -0.034241  0.707008\u001b[0m\n",
      "\u001b[34m..   ..           ...           ...  ...       ...       ...       ...\u001b[0m\n",
      "\u001b[34m180   0   4856.983724   -614.462545  ... -0.803300  0.517730 -0.004388\u001b[0m\n",
      "\u001b[34m181   0  80305.741216 -10439.919311  ...  0.702867 -0.063876  1.477078\u001b[0m\n",
      "\u001b[34m182   1   2760.958312   1791.546966  ... -0.626393 -0.284007 -0.001839\u001b[0m\n",
      "\u001b[34m183   1   3837.360174   -482.340859  ... -0.242682 -0.338915 -0.071272\u001b[0m\n",
      "\u001b[34m184   0   6735.583652   -858.953896  ...  0.434896  0.003691 -0.134524\u001b[0m\n",
      "\u001b[34m[185 rows x 11 columns]\u001b[0m\n",
      "\u001b[34mINFO:root:'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mINFO:root:Beginning training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of positive: 295, number of negative: 134\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Total Bins 1437\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of data points in the train set: 429, number of used features: 10\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.687646 -> initscore=0.789136\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Start training from score 0.789136\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[1]#011train's binary_logloss: 0.591374#011val's binary_logloss: 0.598969\u001b[0m\n",
      "\u001b[34mTraining until validation scores don't improve for 30 rounds\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[2]#011train's binary_logloss: 0.571033#011val's binary_logloss: 0.581269\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[3]#011train's binary_logloss: 0.550966#011val's binary_logloss: 0.565205\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[4]#011train's binary_logloss: 0.544667#011val's binary_logloss: 0.564237\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[5]#011train's binary_logloss: 0.529267#011val's binary_logloss: 0.553372\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[6]#011train's binary_logloss: 0.51454#011val's binary_logloss: 0.541624\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[7]#011train's binary_logloss: 0.502361#011val's binary_logloss: 0.531892\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[8]#011train's binary_logloss: 0.491963#011val's binary_logloss: 0.5241\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[9]#011train's binary_logloss: 0.482175#011val's binary_logloss: 0.518271\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[10]#011train's binary_logloss: 0.476473#011val's binary_logloss: 0.516547\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[11]#011train's binary_logloss: 0.467235#011val's binary_logloss: 0.509349\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[12]#011train's binary_logloss: 0.457528#011val's binary_logloss: 0.501275\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[13]#011train's binary_logloss: 0.452393#011val's binary_logloss: 0.499744\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[14]#011train's binary_logloss: 0.447675#011val's binary_logloss: 0.499954\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[15]#011train's binary_logloss: 0.440764#011val's binary_logloss: 0.497139\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[16]#011train's binary_logloss: 0.43738#011val's binary_logloss: 0.497389\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[17]#011train's binary_logloss: 0.431642#011val's binary_logloss: 0.495554\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[18]#011train's binary_logloss: 0.42904#011val's binary_logloss: 0.496206\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[19]#011train's binary_logloss: 0.423641#011val's binary_logloss: 0.492478\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[20]#011train's binary_logloss: 0.419316#011val's binary_logloss: 0.489646\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[21]#011train's binary_logloss: 0.412355#011val's binary_logloss: 0.48642\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[22]#011train's binary_logloss: 0.406431#011val's binary_logloss: 0.488527\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[23]#011train's binary_logloss: 0.402277#011val's binary_logloss: 0.486377\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[24]#011train's binary_logloss: 0.397114#011val's binary_logloss: 0.488964\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[25]#011train's binary_logloss: 0.393679#011val's binary_logloss: 0.495482\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[26]#011train's binary_logloss: 0.389567#011val's binary_logloss: 0.493597\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[27]#011train's binary_logloss: 0.386279#011val's binary_logloss: 0.493431\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[28]#011train's binary_logloss: 0.38433#011val's binary_logloss: 0.49493\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[29]#011train's binary_logloss: 0.381309#011val's binary_logloss: 0.497504\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[30]#011train's binary_logloss: 0.379107#011val's binary_logloss: 0.496759\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[31]#011train's binary_logloss: 0.376162#011val's binary_logloss: 0.501155\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[32]#011train's binary_logloss: 0.372573#011val's binary_logloss: 0.506996\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[33]#011train's binary_logloss: 0.37022#011val's binary_logloss: 0.512748\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[34]#011train's binary_logloss: 0.369363#011val's binary_logloss: 0.516687\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[35]#011train's binary_logloss: 0.368365#011val's binary_logloss: 0.518772\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[36]#011train's binary_logloss: 0.365107#011val's binary_logloss: 0.521686\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[37]#011train's binary_logloss: 0.360766#011val's binary_logloss: 0.521905\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[38]#011train's binary_logloss: 0.35927#011val's binary_logloss: 0.520375\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[39]#011train's binary_logloss: 0.35737#011val's binary_logloss: 0.520741\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[40]#011train's binary_logloss: 0.355866#011val's binary_logloss: 0.522421\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[41]#011train's binary_logloss: 0.351602#011val's binary_logloss: 0.519767\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[42]#011train's binary_logloss: 0.348859#011val's binary_logloss: 0.519824\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[43]#011train's binary_logloss: 0.347004#011val's binary_logloss: 0.520972\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[44]#011train's binary_logloss: 0.344977#011val's binary_logloss: 0.521083\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[45]#011train's binary_logloss: 0.343715#011val's binary_logloss: 0.522067\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[46]#011train's binary_logloss: 0.339366#011val's binary_logloss: 0.520968\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[47]#011train's binary_logloss: 0.337336#011val's binary_logloss: 0.519219\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[48]#011train's binary_logloss: 0.33551#011val's binary_logloss: 0.523018\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[49]#011train's binary_logloss: 0.333126#011val's binary_logloss: 0.523267\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[50]#011train's binary_logloss: 0.33087#011val's binary_logloss: 0.524626\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[51]#011train's binary_logloss: 0.328695#011val's binary_logloss: 0.524514\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[52]#011train's binary_logloss: 0.326537#011val's binary_logloss: 0.526453\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[53]#011train's binary_logloss: 0.324545#011val's binary_logloss: 0.523531\u001b[0m\n",
      "\u001b[34mEarly stopping, best iteration is:\u001b[0m\n",
      "\u001b[34m[23]#011train's binary_logloss: 0.402277#011val's binary_logloss: 0.486377\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34mINFO:root:Info file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2024-03-27 10:16:13,090 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-27 10:16:37 Uploading - Uploading generated training model\n",
      "2024-03-27 10:16:37 Completed - Training job completed\n",
      "Training seconds: 82\n",
      "Billable seconds: 82\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Define paths for training and validation datasets\n",
    "train_data_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "valid_data_path = f\"s3://{bucket}/{prefix}/valid\"\n",
    "\n",
    "# Create TrainingInput objects for training and validation datasets\n",
    "train_input = TrainingInput(train_data_path, content_type=\"text/csv\")\n",
    "valid_input = TrainingInput(valid_data_path, content_type=\"text/csv\")\n",
    "\n",
    "# Generating a unique name for the training job\n",
    "train_job_name = name_from_base(\"lightgbm-train-job\")\n",
    "\n",
    "# Initiating the training job with training and validation datasets using TrainingInput\n",
    "estimator.fit(\n",
    "    {\"train\": train_input, \"validation\": valid_input},\n",
    "    logs=True,  # Print logs during training\n",
    "    job_name=train_job_name  # Assigning a name to the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c04c7",
   "metadata": {},
   "source": [
    "### Deploying Trained Model as an Endpoint for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc7dd6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-eu-north-1-339713058917/demo-1/output/my-first-example-2024-03-27-10-14-09-762/output/model.tar.gz), script artifact (s3://jumpstart-cache-prod-eu-north-1/source-directory-tarballs/lightgbm/inference/classification/v1.2.2/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-eu-north-1-339713058917/sagemaker-jumpstart-2024-03-27-10-20-29-390/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2024-03-27-10-20-29-390\n",
      "INFO:sagemaker:Creating endpoint-config with name lightgbm-endpoint-2024-03-27-10-20-29-390\n",
      "INFO:sagemaker:Creating endpoint with name lightgbm-endpoint-2024-03-27-10-20-29-390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Defining the instance type for inference\n",
    "inference_instance_type = \"ml.m5.large\"\n",
    "\n",
    "# Generating a unique endpoint name\n",
    "endpoint_name = name_from_base(\"lightgbm-endpoint\")\n",
    "\n",
    "# Retrieving the Docker image URI for the inference image\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,  # Use default region\n",
    "    framework=None,  # No specific framework required\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieving the source script URI for inference\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Deploying the trained model as an endpoint for inference\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,  # Number of instances to start with\n",
    "    instance_type=inference_instance_type,  # Instance type for the endpoint\n",
    "    entry_point=\"inference.py\",  # Script to handle inference requests\n",
    "    image_uri=deploy_image_uri,  # Docker image URI for the endpoint\n",
    "    source_dir=deploy_source_uri,  # Source script URI for the endpoint\n",
    "    endpoint_name=endpoint_name  # Name of the endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b332a",
   "metadata": {},
   "source": [
    "### Make Inference Requests on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df4efd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "client = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "# Define the content type of the input data\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "# Create a buffer to store the downloaded data from S3\n",
    "buffer = io.BytesIO()\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Download the test data file from S3 into the buffer\n",
    "s3.download_fileobj(bucket, os.path.join(prefix, \"test/data.csv\"), buffer)\n",
    "\n",
    "# Invoke the SageMaker endpoint with the downloaded data\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=buffer.getvalue(),\n",
    ")\n",
    "\n",
    "# Read the response from the endpoint and parse it as JSON\n",
    "predictions = json.loads(response[\"Body\"].read())\n",
    "\n",
    "# Extract the probabilities from the predictions\n",
    "probabilities = np.array(predictions[\"probabilities\"])\n",
    "\n",
    "# Determine the predicted classes by selecting the index of the maximum probability for each prediction\n",
    "predictions = probabilities.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425149e",
   "metadata": {},
   "source": [
    "### Deleting Model and Endpoint Resources in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25bdf2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2024-03-27-10-20-29-390\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: lightgbm-endpoint-2024-03-27-10-20-29-390\n",
      "INFO:sagemaker:Deleting endpoint with name: lightgbm-endpoint-2024-03-27-10-20-29-390\n"
     ]
    }
   ],
   "source": [
    "# Delete the model\n",
    "predictor.delete_model()\n",
    "\n",
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b4fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

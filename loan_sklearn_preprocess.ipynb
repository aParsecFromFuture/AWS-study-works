{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e100b0",
   "metadata": {},
   "source": [
    "### Setting Up AWS Environment for SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2a6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Getting the execution role for SageMaker\n",
    "aws_role = get_execution_role()\n",
    "\n",
    "# Getting the AWS region using the boto3 session\n",
    "aws_region = boto3.Session().region_name\n",
    "\n",
    "# Creating a SageMaker session\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Getting the default S3 bucket associated with the SageMaker session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Defining a prefix for the S3 location\n",
    "prefix = \"catboost-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d92488",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline for Loan Eligibility Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e76cdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Ignore warnings related to data conversion\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "# Define columns of interest\n",
    "columns = [\n",
    "    \"Loan_Status\",\n",
    "    \"ApplicantIncome\", \n",
    "    \"CoapplicantIncome\",\n",
    "    \"LoanAmount\", \n",
    "    \"Loan_Amount_Term\",\n",
    "    \"Gender\", \n",
    "    \"Married\", \n",
    "    \"Dependents\", \n",
    "    \"Education\",\n",
    "    \"Credit_History\",\n",
    "    \"Property_Area\",\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received Arguments: {}\".format(args))\n",
    "    \n",
    "    # Load input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input/loan-train.csv\")\n",
    "    df = pd.read_csv(input_data_path, usecols=columns)\n",
    "    \n",
    "    # Map target labels to binary values\n",
    "    df[\"Loan_Status\"] = df[\"Loan_Status\"].map({\"Y\": 1, \"N\": 0})\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print(\"Splitting data into train and test sets with ratio {}\".format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.drop(columns=[\"Loan_Status\"]), df[\"Loan_Status\"], test_size=split_ratio, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Define preprocessing pipeline\n",
    "    preprocess = make_column_transformer(\n",
    "        (Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "            (\"disretizer\", KBinsDiscretizer(encode=\"ordinal\", n_bins=5)),\n",
    "        ]), [\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\"]),\n",
    "        (Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "            (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=1024)),\n",
    "        ]), [\"Gender\", \"Married\", \"Dependents\", \"Education\"]),\n",
    "        (Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), [\"Property_Area\"]),\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing pipeline\n",
    "    print(\"Running preprocessing and feature engineering transformations\")\n",
    "    X_train = preprocess.fit_transform(X_train)\n",
    "    X_test = preprocess.transform(X_test)\n",
    "    \n",
    "    # Display shapes of train and test data after preprocessing\n",
    "    print(\"Train data shape after preprocessing: {}\".format(X_train.shape))\n",
    "    print(\"Test data shape after preprocessing: {}\".format(X_test.shape))\n",
    "    \n",
    "    # Concatenate target variable with features\n",
    "    train = np.column_stack([y_train, X_train])\n",
    "    test = np.column_stack([y_test, X_test])\n",
    "    \n",
    "    # Define paths to save processed data\n",
    "    train_data_output_path = os.path.join(\"/opt/ml/processing/train\", \"data.csv\")\n",
    "    test_data_output_path = os.path.join(\"/opt/ml/processing/validation\", \"data.csv\")\n",
    "    \n",
    "    # Save processed train and test data\n",
    "    print(\"Saving training data to {}\".format(train_data_output_path))\n",
    "    pd.DataFrame(train).to_csv(train_data_output_path, header=False, index=False)\n",
    "    \n",
    "    print(\"Saving test data to {}\".format(test_data_output_path))\n",
    "    pd.DataFrame(test).to_csv(test_data_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0410980",
   "metadata": {},
   "source": [
    "### SageMaker SKLearn Processor for Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8276df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2024-03-28-09-46-06-395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\u001b[34mReceived Arguments: Namespace(train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mSplitting data into train and test sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_discretization.py:230: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_discretization.py:230: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (491, 11)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (123, 11)\u001b[0m\n",
      "\u001b[34mSaving training data to /opt/ml/processing/train/data.csv\u001b[0m\n",
      "\u001b[34mSaving test data to /opt/ml/processing/validation/data.csv\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Define S3 bucket and prefix for input data\n",
    "bucket = \"farukcan-loan-eligibility\"\n",
    "prefix = \"demo-1/source\"\n",
    "source_s3_path = f\"s3://{bucket}/{prefix}/loan-train.csv\"\n",
    "\n",
    "# Initialize SKLearnProcessor\n",
    "sklearn_preprocessor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=aws_role,  # Define AWS IAM role for accessing resources\n",
    "    instance_type=\"ml.m5.xlarge\",  # Choose instance type for processing\n",
    "    instance_count=1,  # Number of instances to use for processing\n",
    ")\n",
    "\n",
    "# Run SKLearnProcessor with defined parameters\n",
    "sklearn_preprocessor.run(\n",
    "    code=\"preprocessing.py\",  # Script to be executed\n",
    "    inputs=[ProcessingInput(source=source_s3_path, destination=\"/opt/ml/processing/input\")],  # Define input data location\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),  # Define output location for training data\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),  # Define output location for validation data\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],  # Pass arguments to the preprocessing script\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf77a66",
   "metadata": {},
   "source": [
    "### Retrieving Processed Data URIs from SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3c6a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information about the latest processing job\n",
    "processing_job_description = sklearn_preprocessor.jobs[-1].describe()\n",
    "\n",
    "# Extract output information from the processing job description\n",
    "outputs = processing_job_description[\"ProcessingOutputConfig\"][\"Outputs\"]\n",
    "\n",
    "# Extract URIs for processed train and test data from the output information\n",
    "processed_train_data_uri = outputs[0][\"S3Output\"][\"S3Uri\"]\n",
    "processed_test_data_uri = outputs[1][\"S3Output\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c14ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
